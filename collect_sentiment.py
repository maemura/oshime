#!/usr/bin/env python3
"""
collect_sentiment.py - ãƒãƒ¼ã‚±ãƒƒãƒˆã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
YouTubeæ ªç³»ãƒãƒ£ãƒ³ãƒãƒ«ã®æœ€æ–°å‹•ç”»ã‹ã‚‰ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆã‚’æŠ½å‡ºã—ã€
sentiment_data/YYYY-MM-DD.json ã«è“„ç©ã™ã‚‹ã€‚

Usage:
  python collect_sentiment.py

ç’°å¢ƒå¤‰æ•°:
  YOUTUBE_API_KEY   - YouTube Data API v3 ã‚­ãƒ¼ï¼ˆå¿…é ˆï¼‰
  ANTHROPIC_API_KEY - Claude API ã‚­ãƒ¼ï¼ˆä»»æ„ã€‚ãªã‘ã‚Œã°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã®ã¿ï¼‰
"""

import os
import sys
import json
import re
from datetime import datetime, timedelta
from pathlib import Path

# â”€â”€ å®šæ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CHANNELS = {
    # ãƒãƒ£ãƒ³ãƒãƒ«ID: è¡¨ç¤ºå
    "UCkKVLw3kFsYmEwjRafdFjkg": "å¾Œè—¤é”ä¹Ÿ",
    "UCFXl12dZUPaiolwPMIbascA": "é«˜æ©‹ãƒ€ãƒ³",
    "UCtEpOqXeDFRy3jhJB2GQGOQ": "ã‚¨ãƒŸãƒ³ãƒ¦ãƒ«ãƒã‚º",
    "UCQPPXy9LCznUQHHG_kh6Bpg": "ãƒãƒ•ã‚§ãƒƒãƒˆå¤ªéƒ",
    "UCLEbHAkkSFGbCiPosa0qTMg": "SBIè¨¼åˆ¸ãƒ“ã‚¸ãƒã‚¹ãƒ‰ãƒ©ã‚¤ãƒ–",
}

# éå»Næ—¥ä»¥å†…ã®å‹•ç”»ã®ã¿å–å¾—
MAX_AGE_DAYS = 2

# ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜å…ˆ
SENTIMENT_DIR = Path("sentiment_data")

# Claude APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
ANTHROPIC_API_URL = "https://api.anthropic.com/v1/messages"

# â”€â”€ YouTube Data API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_latest_videos(api_key, channel_id, max_results=3):
    """ãƒãƒ£ãƒ³ãƒãƒ«ã®æœ€æ–°å‹•ç”»ã‚’å–å¾—"""
    import urllib.request
    import urllib.parse

    # éå»MAX_AGE_DAYSä»¥å†…
    after = (datetime.utcnow() - timedelta(days=MAX_AGE_DAYS)).strftime("%Y-%m-%dT%H:%M:%SZ")

    params = urllib.parse.urlencode({
        "part": "snippet",
        "channelId": channel_id,
        "maxResults": max_results,
        "order": "date",
        "type": "video",
        "publishedAfter": after,
        "key": api_key,
    })

    url = f"https://www.googleapis.com/youtube/v3/search?{params}"

    try:
        req = urllib.request.Request(url)
        with urllib.request.urlopen(req, timeout=15) as resp:
            data = json.loads(resp.read().decode())

        videos = []
        for item in data.get("items", []):
            vid = item["id"].get("videoId")
            if vid:
                videos.append({
                    "video_id": vid,
                    "title": item["snippet"]["title"],
                    "published": item["snippet"]["publishedAt"],
                    "url": f"https://www.youtube.com/watch?v={vid}",
                })
        return videos
    except Exception as e:
        print(f"  âš  YouTube API ã‚¨ãƒ©ãƒ¼: {e}")
        return []


# â”€â”€ Transcriptå–å¾— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_transcript(video_id):
    """YouTubeã®è‡ªå‹•å­—å¹•ã‚’å–å¾—"""
    try:
        from youtube_transcript_api import YouTubeTranscriptApi
        # æ—¥æœ¬èª â†’ è‹±èª â†’ è‡ªå‹•ç”Ÿæˆã®é †ã§è©¦è¡Œ
        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)

        transcript = None
        for lang in ["ja", "en"]:
            try:
                transcript = transcript_list.find_transcript([lang])
                break
            except:
                continue

        if not transcript:
            # è‡ªå‹•ç”Ÿæˆã‚’è©¦ã™
            try:
                transcript = transcript_list.find_generated_transcript(["ja", "en"])
            except:
                return None

        entries = transcript.fetch()
        text = " ".join([e.text if isinstance(e, dict) is False else e.get("text", "") for e in entries])

        # entries ãŒã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆ
        if not text.strip():
            text = " ".join([str(e) for e in entries])

        return text[:8000]  # æœ€å¤§8000æ–‡å­—ï¼ˆã‚³ã‚¹ãƒˆåˆ¶å¾¡ï¼‰

    except Exception as e:
        print(f"    âš  Transcriptå–å¾—å¤±æ•—: {e}")
        return None


# â”€â”€ Claude APIã§è¦ç´„ãƒ»ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆæŠ½å‡º â”€â”€â”€â”€
def extract_sentiment_claude(text, channel_name, video_title, api_key):
    """Claude APIã§ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆã‚’æŠ½å‡º"""
    import urllib.request

    prompt = f"""ä»¥ä¸‹ã¯YouTubeã®æ ªå¼æŠ•è³‡ãƒãƒ£ãƒ³ãƒãƒ«ã€Œ{channel_name}ã€ã®å‹•ç”»ã€Œ{video_title}ã€ã®æ–‡å­—èµ·ã“ã—ã§ã™ã€‚

ã“ã®å‹•ç”»ã‹ã‚‰ä»¥ä¸‹ã®æƒ…å ±ã‚’JSONå½¢å¼ã§æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

1. topics: è©±é¡Œã«ãªã£ã¦ã„ã‚‹é …ç›®ã®ãƒªã‚¹ãƒˆã€‚å„é …ç›®ã«ä»¥ä¸‹ã‚’å«ã‚€ï¼š
   - topic: ãƒ†ãƒ¼ãƒåï¼ˆéŠ˜æŸ„åã€ã‚»ã‚¯ã‚¿ãƒ¼åã€çµŒæ¸ˆã‚¤ãƒ™ãƒ³ãƒˆåãªã©ï¼‰
   - category: "stock"ï¼ˆå€‹åˆ¥éŠ˜æŸ„ï¼‰, "sector"ï¼ˆã‚»ã‚¯ã‚¿ãƒ¼ï¼‰, "macro"ï¼ˆãƒã‚¯ãƒ­çµŒæ¸ˆï¼‰, "event"ï¼ˆã‚¤ãƒ™ãƒ³ãƒˆï¼‰ã®ã„ãšã‚Œã‹
   - sentiment: "bullish"ï¼ˆå¼·æ°—ï¼‰, "bearish"ï¼ˆå¼±æ°—ï¼‰, "neutral"ï¼ˆä¸­ç«‹ï¼‰, "hype"ï¼ˆéç†±ï¼‰, "fear"ï¼ˆææ€–ï¼‰ã®ã„ãšã‚Œã‹
   - confidence: 0.0ã€œ1.0ï¼ˆç¢ºä¿¡åº¦ï¼‰
   - summary: ä¸€è¨€è¦ç´„ï¼ˆ30æ–‡å­—ä»¥å†…ï¼‰

2. overall_mood: å‹•ç”»å…¨ä½“ã®ãƒ ãƒ¼ãƒ‰ï¼ˆ"bullish", "bearish", "neutral", "cautious", "mixed"ã®ã„ãšã‚Œã‹ï¼‰

3. key_quote: å°è±¡çš„ãªä¸€è¨€ï¼ˆ50æ–‡å­—ä»¥å†…ï¼‰

JSONã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã®ãƒãƒƒã‚¯ã‚¯ã‚©ãƒ¼ãƒˆã¯ä¸è¦ã§ã™ã€‚

--- æ–‡å­—èµ·ã“ã— ---
{text[:5000]}
"""

    body = json.dumps({
        "model": "claude-sonnet-4-20250514",
        "max_tokens": 1500,
        "messages": [{"role": "user", "content": prompt}],
    }).encode()

    req = urllib.request.Request(
        ANTHROPIC_API_URL,
        data=body,
        headers={
            "Content-Type": "application/json",
            "x-api-key": api_key,
            "anthropic-version": "2023-06-01",
        },
    )

    try:
        with urllib.request.urlopen(req, timeout=30) as resp:
            result = json.loads(resp.read().decode())

        text_content = result["content"][0]["text"]
        # JSONéƒ¨åˆ†ã‚’æŠ½å‡º
        text_content = text_content.strip()
        if text_content.startswith("```"):
            text_content = re.sub(r"^```\w*\n?", "", text_content)
            text_content = re.sub(r"\n?```$", "", text_content)

        return json.loads(text_content)

    except Exception as e:
        print(f"    âš  Claude API ã‚¨ãƒ©ãƒ¼: {e}")
        return None


# â”€â”€ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ â”€â”€â”€â”€â”€â”€
def extract_sentiment_keywords(text, video_title):
    """Claude APIãŒä½¿ãˆãªã„å ´åˆã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º"""
    bullish_words = ["ä¸Šæ˜‡", "è²·ã„", "å¼·æ°—", "æœŸå¾…", "å¥½æ±ºç®—", "ä¸ŠãŒã‚‹", "ãƒãƒ£ãƒ³ã‚¹", "æŠ¼ã—ç›®", "åç™º", "é«˜é…å½“"]
    bearish_words = ["ä¸‹è½", "å£²ã‚Š", "å¼±æ°—", "æš´è½", "ãƒªã‚¹ã‚¯", "ä¸‹ãŒã‚‹", "å±é™º", "æåˆ‡", "å¤©äº•", "éç†±"]

    # éŠ˜æŸ„åãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º
    stock_patterns = [
        r"æ—¥çµŒå¹³å‡|æ—¥çµŒ225|TOPIX",
        r"NVDA|NVIDIA|ã‚¨ãƒŒãƒ“ãƒ‡ã‚£ã‚¢",
        r"åŠå°ä½“|AIéŠ˜æŸ„|SaaS",
        r"ãƒˆãƒ¨ã‚¿|ã‚½ãƒ‹ãƒ¼|ä»»å¤©å ‚|ä¿¡è¶ŠåŒ–å­¦|ã‚¤ãƒ“ãƒ‡ãƒ³",
        r"S&P500|ãƒ€ã‚¦|ãƒŠã‚¹ãƒ€ãƒƒã‚¯",
        r"é…å½“|å„ªå¾…|é«˜é…å½“",
        r"å††å®‰|å††é«˜|ãƒ‰ãƒ«å††",
        r"é‡‘åˆ©|FRB|æ—¥éŠ€",
    ]

    topics = []
    for pattern in stock_patterns:
        matches = re.findall(pattern, text)
        if matches:
            word = matches[0]
            # å‰å¾Œã®æ–‡è„ˆã§ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆæ¨å®š
            bull = sum(1 for bw in bullish_words if bw in text)
            bear = sum(1 for bw in bearish_words if bw in text)
            sentiment = "bullish" if bull > bear else "bearish" if bear > bull else "neutral"

            topics.append({
                "topic": word,
                "category": "macro" if any(w in word for w in ["æ—¥çµŒ", "S&P", "å††", "é‡‘åˆ©", "FRB", "æ—¥éŠ€"]) else "stock",
                "sentiment": sentiment,
                "confidence": 0.4,
                "summary": f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º: {word}",
            })

    bull_total = sum(1 for bw in bullish_words if bw in text)
    bear_total = sum(1 for bw in bearish_words if bw in text)
    mood = "bullish" if bull_total > bear_total * 1.3 else "bearish" if bear_total > bull_total * 1.3 else "mixed"

    return {
        "topics": topics[:10],
        "overall_mood": mood,
        "key_quote": video_title[:50],
    }


# â”€â”€ ãƒ¡ã‚¤ãƒ³å‡¦ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    yt_key = os.environ.get("YOUTUBE_API_KEY", "")
    claude_key = os.environ.get("ANTHROPIC_API_KEY", "")

    if not yt_key:
        print("âŒ YOUTUBE_API_KEY ãŒæœªè¨­å®šã§ã™")
        sys.exit(1)

    use_claude = bool(claude_key)
    if use_claude:
        print("ğŸ¤– Claude API æœ‰åŠ¹ â€” AIè¦ç´„ãƒ¢ãƒ¼ãƒ‰")
    else:
        print("ğŸ“ Claude API æœªè¨­å®š â€” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºãƒ¢ãƒ¼ãƒ‰")

    today = datetime.now().strftime("%Y-%m-%d")
    SENTIMENT_DIR.mkdir(exist_ok=True)

    all_entries = []
    video_count = 0

    print(f"\n{'='*50}")
    print(f"ğŸ“¡ MARKET SENTIMENT COLLECTOR â€” {today}")
    print(f"{'='*50}")

    for ch_id, ch_name in CHANNELS.items():
        print(f"\nğŸ“º {ch_name}")

        videos = fetch_latest_videos(yt_key, ch_id, max_results=3)
        if not videos:
            print(f"  â†’ æ–°ã—ã„å‹•ç”»ãªã—")
            continue

        for v in videos:
            print(f"  ğŸ¬ {v['title'][:50]}...")
            video_count += 1

            # Transcriptå–å¾—
            transcript = fetch_transcript(v["video_id"])
            if not transcript:
                print(f"    â†’ å­—å¹•å–å¾—ã§ããšã€ã‚¹ã‚­ãƒƒãƒ—")
                continue

            print(f"    â†’ å­—å¹•å–å¾—OKï¼ˆ{len(transcript)}æ–‡å­—ï¼‰")

            # ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆæŠ½å‡º
            if use_claude:
                result = extract_sentiment_claude(transcript, ch_name, v["title"], claude_key)
            else:
                result = None

            if not result:
                result = extract_sentiment_keywords(transcript, v["title"])

            # ã‚¨ãƒ³ãƒˆãƒªä½œæˆ
            if result and result.get("topics"):
                for topic in result["topics"]:
                    all_entries.append({
                        "source": "youtube",
                        "channel": ch_name,
                        "video_title": v["title"][:80],
                        "video_url": v["url"],
                        "topic": topic.get("topic", ""),
                        "category": topic.get("category", "macro"),
                        "sentiment": topic.get("sentiment", "neutral"),
                        "confidence": topic.get("confidence", 0.5),
                        "summary": topic.get("summary", ""),
                    })

                print(f"    â†’ {len(result['topics'])}ä»¶ã®ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡º")
                print(f"    â†’ å…¨ä½“ãƒ ãƒ¼ãƒ‰: {result.get('overall_mood', '?')}")
                if result.get("key_quote"):
                    print(f"    â†’ ğŸ’¬ {result['key_quote']}")

    # â”€â”€ çµæœä¿å­˜ â”€â”€
    output = {
        "date": today,
        "collected_at": datetime.now().strftime("%Y-%m-%d %H:%M"),
        "mode": "claude" if use_claude else "keywords",
        "channels_checked": len(CHANNELS),
        "videos_processed": video_count,
        "entry_count": len(all_entries),
        "entries": all_entries,
        "summary": {
            "by_source": {},
            "by_topic": {},
            "hot_words": [],
        },
    }

    # ã‚µãƒãƒªãƒ¼é›†è¨ˆ
    topic_counts = {}
    for e in all_entries:
        t = e["topic"]
        if t not in topic_counts:
            topic_counts[t] = {"count": 0, "sentiments": []}
        topic_counts[t]["count"] += 1
        topic_counts[t]["sentiments"].append(e["sentiment"])

    # ãƒˆãƒ”ãƒƒã‚¯ã”ã¨ã®å¤šæ•°æ±ºã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ
    for t, data in topic_counts.items():
        sents = data["sentiments"]
        from collections import Counter
        most_common = Counter(sents).most_common(1)[0][0]
        output["summary"]["by_topic"][t] = {
            "count": data["count"],
            "sentiment": most_common,
        }

    # ãƒ›ãƒƒãƒˆãƒ¯ãƒ¼ãƒ‰ï¼ˆå‡ºç¾é »åº¦é †ï¼‰
    output["summary"]["hot_words"] = [
        t for t, _ in sorted(topic_counts.items(), key=lambda x: -x[1]["count"])
    ][:15]

    # JSONä¿å­˜
    out_path = SENTIMENT_DIR / f"{today}.json"
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    print(f"\n{'='*50}")
    print(f"âœ… å®Œäº†: {len(all_entries)}ä»¶ã®ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆã‚’åé›†")
    print(f"ğŸ“ ä¿å­˜å…ˆ: {out_path}")
    print(f"ğŸ”¥ ãƒ›ãƒƒãƒˆãƒ¯ãƒ¼ãƒ‰: {', '.join(output['summary']['hot_words'][:5])}")
    print(f"{'='*50}")

    # â”€â”€ Discordé€šçŸ¥ â”€â”€
    webhook_url = os.environ.get("DISCORD_WEBHOOK_URL", "")
    if webhook_url and all_entries:
        try:
            import urllib.request
            hot = ', '.join(output['summary']['hot_words'][:5])

            # ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆé›†è¨ˆ
            bull = sum(1 for e in all_entries if e["sentiment"] in ("bullish",))
            bear = sum(1 for e in all_entries if e["sentiment"] in ("bearish", "fear"))
            msg = (
                f"ğŸ“¡ **ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåé›†å®Œäº†** ({today})\n"
                f"å‹•ç”»: {video_count}æœ¬ / ãƒˆãƒ”ãƒƒã‚¯: {len(all_entries)}ä»¶\n"
                f"å¼·æ°—: {bull} / å¼±æ°—: {bear}\n"
                f"ğŸ”¥ {hot}"
            )

            body = json.dumps({"content": msg}).encode()
            req = urllib.request.Request(
                webhook_url,
                data=body,
                headers={"Content-Type": "application/json"},
            )
            urllib.request.urlopen(req, timeout=10)
            print("ğŸ“¢ Discordé€šçŸ¥é€ä¿¡")
        except Exception as e:
            print(f"âš  Discordé€šçŸ¥å¤±æ•—: {e}")


if __name__ == "__main__":
    main()
